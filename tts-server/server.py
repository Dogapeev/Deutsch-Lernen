# –§–∞–π–ª: tts-server/server.py
# –í–ï–†–°–ò–Ø 1.3.1: –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º

import os
import json
import hashlib
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Set
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from gtts import gTTS
import logging
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import threading
from datetime import datetime
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import signal
import sys

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
app = Flask(__name__)
CORS(app)
limiter = Limiter(get_remote_address, app=app, default_limits=["200 per day", "50 per hour"])

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
PRODUCTION = os.getenv('PRODUCTION', 'false').lower() == 'true'
AUDIO_DIR = "audio_cache"
VOCABULARIES_DIR = "vocabularies"
CACHE_MANIFEST = "cache_manifest.json"
CONFIG_FILE = "auto_config.json"
ACCESS_STATS_FILE = "access_stats.json"
SUPPORTED_LANGUAGES = {'de', 'ru', 'en', 'fr', 'es'}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_level = logging.WARNING if PRODUCTION else logging.INFO
logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
start_time = time.time()

class AutoVocabularySystem:
    def __init__(self):
        self.audio_dir = Path(AUDIO_DIR)
        self.vocabularies_dir = Path(VOCABULARIES_DIR)
        self.config = self.load_config()
        self.protected_hashes = set()
        self.vocabulary_registry = {}
        self.file_observer = None
        self.processing_queue = asyncio.Queue()
        self.last_cleanup_time = time.time()
        self.gtts_semaphore = asyncio.Semaphore(3)

        os.makedirs(AUDIO_DIR, exist_ok=True)
        os.makedirs(VOCABULARIES_DIR, exist_ok=True)
        self.load_manifest()
        
        self.loop = asyncio.new_event_loop()
        self.background_thread = threading.Thread(target=self.run_background_processor, daemon=True)
        self.background_thread.start()
        
        if self.config.get('auto_watch_enabled', True):
            self.start_file_watcher()

    def load_config(self):
        default_config = {
            "auto_watch_enabled": True,
            "auto_process_on_change": True,
            "auto_process_on_startup": True,
            "retry_failed": True,
            "cleanup_on_startup": False,
            "auto_cleanup_enabled": True,
            "cleanup_interval_hours": 24,
            "min_access_count_protect": 2,
            "max_cache_files": 1000,
            "max_cache_size_mb": 500,
            "check_interval_seconds": 300,
            "supported_extensions": [".json"],
            "exclude_patterns": [
                ".*",      # —Ñ–∞–π–ª—ã –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ç–æ—á–∫–∏ (.gitignore, .DS_Store –∏ —Ç.–¥.)
                "~*",      # –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (~vocabulary.json)
                "*~",      # backup —Ñ–∞–π–ª—ã (vocabulary.json~)
                "*.bak",   # backup —Ñ–∞–π–ª—ã (vocabulary.json.bak)
                "*.tmp"    # –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (vocabulary.json.tmp)
            ]
        }
        
        try:
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    loaded_config.pop('_comments', None)
                    default_config.update(loaded_config)
                logger.info("üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
            else:
                config_with_comments = {
                    **default_config,
                    "_comments": {
                        "exclude_patterns": [
                            "–ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏:",
                            "  '.*' - —Ñ–∞–π–ª—ã –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ç–æ—á–∫–∏",
                            "  '~*' - —Ñ–∞–π–ª—ã –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ç–∏–ª—å–¥—ã", 
                            "  '*~' - —Ñ–∞–π–ª—ã –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–µ—Å—è —Ç–∏–ª—å–¥–æ–π",
                            "  '*.bak' - backup —Ñ–∞–π–ª—ã",
                            "  '*.tmp' - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã",
                        ]
                    }
                }
                
                with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                    json.dump(config_with_comments, f, indent=2, ensure_ascii=False)
                logger.info("üîß –°–æ–∑–¥–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}", exc_info=not PRODUCTION)
        
        logger.info(f"üìã –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–∫–ª—é—á–µ–Ω–∏–π: {default_config['exclude_patterns']}")
        return default_config

    def load_manifest(self):
        try:
            if os.path.exists(CACHE_MANIFEST):
                with open(CACHE_MANIFEST, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.protected_hashes = set(data.get('protected_hashes', []))
                    self.vocabulary_registry = data.get('vocabularies', {})
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞: {e}", exc_info=not PRODUCTION)

    def save_manifest(self):
        try:
            data = {'protected_hashes': list(self.protected_hashes), 'vocabularies': self.vocabulary_registry, 'last_updated': datetime.now().isoformat()}
            with open(CACHE_MANIFEST, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞: {e}", exc_info=not PRODUCTION)

    def scan_vocabularies(self, auto_process=None):
        logger.info("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π...")
        if auto_process is None: 
            auto_process = self.config.get('auto_process_on_startup', True)
        
        vocab_files = []
        if not self.vocabularies_dir.exists():
            logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–ª–æ–≤–∞—Ä–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.vocabularies_dir}")
            return []
            
        for ext in self.config['supported_extensions']:
            vocab_files.extend(self.vocabularies_dir.glob(f"*{ext}"))
        
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(vocab_files)}")
        
        new_or_changed = []
        
        for vocab_file in vocab_files:
            vocab_name = vocab_file.stem
            
            # --- –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –ò–°–ö–õ–Æ–ß–ï–ù–ò–ô –ò –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ---
            should_exclude = False
            matched_pattern = "" # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–≤–ø–∞–≤—à–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
            for pattern in self.config['exclude_patterns']:
                is_match = False
                if pattern.startswith('*') and vocab_file.name.endswith(pattern[1:]):
                    is_match = True
                elif pattern.endswith('*') and vocab_file.name.startswith(pattern[:-1]):
                    is_match = True
                elif '*' not in pattern and pattern == vocab_file.name:
                    is_match = True
                
                if is_match:
                    should_exclude = True
                    matched_pattern = pattern
                    break
            
            if should_exclude:
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª –ø–æ –ø—Ä–∞–≤–∏–ª—É –∏—Å–∫–ª—é—á–µ–Ω–∏—è '{matched_pattern}': {vocab_file.name}")
                continue
            # --- –ö–û–ù–ï–¶ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø ---
                
            try:
                current_mtime = vocab_file.stat().st_mtime
                
                needs_update = (
                    vocab_name not in self.vocabulary_registry or 
                    self.vocabulary_registry[vocab_name].get('last_modified', 0) < current_mtime
                )
                
                if needs_update:
                    logger.info(f"üìñ –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å. –ó–∞–≥—Ä—É–∑–∫–∞: {vocab_name}")
                    
                    with open(vocab_file, 'r', encoding='utf-8') as f:
                        vocab_data = json.load(f)
                    
                    if not isinstance(vocab_data, list):
                        logger.error(f"‚ùå –°–ª–æ–≤–∞—Ä—å {vocab_name} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º (list)")
                        continue
                        
                    self.vocabulary_registry[vocab_name] = {
                        'file_path': str(vocab_file),
                        'word_count': len(vocab_data),
                        'last_modified': current_mtime,
                        'status': 'detected',
                        'detection_time': datetime.now().isoformat()
                    }
                    
                    new_or_changed.append(vocab_name)
                    logger.info(f"‚úÖ –°–ª–æ–≤–∞—Ä—å {vocab_name} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω/–æ–±–Ω–æ–≤–ª–µ–Ω ({len(vocab_data)} —Å–ª–æ–≤)")
                
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –≤ —Ñ–∞–π–ª–µ {vocab_file.name}: {e}")
            except Exception as e:
                logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞—Ä—è {vocab_file.name}: {e}", exc_info=not PRODUCTION)
        
        if new_or_changed:
            self.save_manifest()
            logger.info(f"üíæ –†–µ–µ—Å—Ç—Ä –æ–±–Ω–æ–≤–ª–µ–Ω. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π: {len(new_or_changed)}")
        
        if auto_process and new_or_changed:
            for vocab_name in new_or_changed:
                if hasattr(self, 'loop') and self.loop.is_running():
                    asyncio.run_coroutine_threadsafe(
                        self.add_to_processing_queue(vocab_name), 
                        self.loop
                    )
            logger.info(f"üîÑ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∞—É–¥–∏–æ: {new_or_changed}")
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä–µ–π –≤ —Ä–µ–µ—Å—Ç—Ä–µ: {len(self.vocabulary_registry)}")
        return new_or_changed

    async def add_to_processing_queue(self, vocab_name):
        await self.processing_queue.put({'action': 'pregenerate', 'vocab_name': vocab_name})

    def is_file_protected(self, filename: str) -> bool:
        return filename.replace('.mp3', '') in self.protected_hashes

    def smart_cleanup(self, force: bool = False):
        audio_files = list(self.audio_dir.glob("*.mp3"))
        total_size_mb = sum(f.stat().st_size for f in audio_files) / (1024 * 1024) if audio_files else 0
        max_files, max_size_mb = self.config.get('max_cache_files', 1000), self.config.get('max_cache_size_mb', 500)
        
        if not (force or len(audio_files) > max_files or total_size_mb > max_size_mb):
            return {"message": "–û—á–∏—Å—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è", "total_files": len(audio_files)}
        
        logger.info(f"üßπ –ù–∞—á–∏–Ω–∞–µ–º —É–º–Ω—É—é –æ—á–∏—Å—Ç–∫—É: {len(audio_files)} —Ñ–∞–π–ª–æ–≤, {total_size_mb:.1f} –ú–ë")
        access_stats = self.load_access_stats()
        
        all_protected_hashes = self.protected_hashes
        orphan_files = [f for f in audio_files if f.stem not in all_protected_hashes]
        deleted_orphans = 0
        for f in orphan_files:
            try:
                f.unlink()
                deleted_orphans += 1
                if f.name in access_stats: del access_stats[f.name]
            except Exception as e: 
                logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {f}: {e}")
        
        self.save_access_stats(access_stats)
        self.last_cleanup_time = time.time()
        logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£–¥–∞–ª–µ–Ω–æ 'orphan' —Ñ–∞–π–ª–æ–≤: {deleted_orphans}")
        return {
            "message": "–£–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", 
            "deleted_files": {"orphans": deleted_orphans}
        }

    def load_access_stats(self):
        if not os.path.exists(ACCESS_STATS_FILE): return {}
        try:
            with open(ACCESS_STATS_FILE, 'r', encoding='utf-8') as f: return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}", exc_info=not PRODUCTION)
        return {}

    def save_access_stats(self, stats):
        try:
            with open(ACCESS_STATS_FILE, 'w', encoding='utf-8') as f: json.dump(stats, f, indent=2)
        except IOError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}", exc_info=not PRODUCTION)

    def record_file_access(self, filename):
        stats = self.load_access_stats()
        now = time.time()
        file_stats = stats.get(filename, {'access_count': 0, 'created': now})
        file_stats['access_count'] += 1
        file_stats['last_access'] = now
        stats[filename] = file_stats
        self.save_access_stats(stats)
    
    def run_background_processor(self):
        asyncio.set_event_loop(self.loop)
        try:
            self.loop.run_until_complete(self.background_processor())
        finally:
            self.loop.close()

    async def background_processor(self):
        logger.info("üîÑ –§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–ø—É—â–µ–Ω")
        while True:
            try:
                task = await asyncio.wait_for(self.processing_queue.get(), timeout=self.config['check_interval_seconds'])
                if isinstance(task, asyncio.CancelledError): break
                if task['action'] == 'pregenerate':
                    vocab_name = task['vocab_name']
                    logger.info(f"üéµ –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è: {vocab_name}")
                    try:
                        result = await self.pregenerate_vocabulary_audio(vocab_name)
                        logger.info(f"‚úÖ –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è {vocab_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {result}")
                        if vocab_name in self.vocabulary_registry:
                            self.vocabulary_registry[vocab_name]['status'] = 'ready'
                            self.vocabulary_registry[vocab_name]['last_processed'] = datetime.now().isoformat()
                            self.save_manifest()
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è {vocab_name}: {e}", exc_info=not PRODUCTION)
                        if vocab_name in self.vocabulary_registry:
                            self.vocabulary_registry[vocab_name]['status'] = 'failed'; self.vocabulary_registry[vocab_name]['last_error'] = str(e)
                self.processing_queue.task_done()
            except asyncio.TimeoutError:
                if self.config.get('auto_cleanup_enabled', True):
                    cleanup_interval = self.config.get('cleanup_interval_hours', 24) * 3600
                    if time.time() - self.last_cleanup_time > cleanup_interval:
                        self.smart_cleanup()
            except asyncio.CancelledError:
                logger.info("–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è.")
                break
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ: {e}", exc_info=not PRODUCTION)
                await asyncio.sleep(10)

    def start_file_watcher(self):
        class VocabularyFileHandler(FileSystemEventHandler):
            def __init__(self, system_ref): 
                self.system = system_ref
                self.timer = None
            
            def on_any_event(self, event):
                if not event.is_directory and any(event.src_path.endswith(ext) for ext in self.system.config['supported_extensions']):
                    if self.timer: self.timer.cancel()
                    logger.info(f"üìÅ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Å–æ–±—ã—Ç–∏–µ —Ñ–∞–π–ª–∞: {event.src_path}. –û–∂–∏–¥–∞–Ω–∏–µ 2 —Å–µ–∫...")
                    self.timer = threading.Timer(2.0, self.system.scan_vocabularies, args=[True])
                    self.timer.start()

        self.file_observer = Observer()
        self.file_observer.schedule(VocabularyFileHandler(self), str(self.vocabularies_dir), recursive=True)
        self.file_observer.start()
        logger.info(f"üëÅÔ∏è –ó–∞–ø—É—â–µ–Ω–æ —Å–ª–µ–∂–µ–Ω–∏–µ –∑–∞ –ø–∞–ø–∫–æ–π: {self.vocabularies_dir}")
        
    def stop_file_watcher(self):
        if self.file_observer and self.file_observer.is_alive(): 
            self.file_observer.stop()
            self.file_observer.join()
            logger.info("üëÅÔ∏è –°–ª–µ–∂–µ–Ω–∏–µ –∑–∞ —Ñ–∞–π–ª–∞–º–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

    async def pregenerate_vocabulary_audio(self, vocab_name: str):
        if vocab_name not in self.vocabulary_registry: 
            raise ValueError(f"–°–ª–æ–≤–∞—Ä—å {vocab_name} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(self.vocabulary_registry[vocab_name]['file_path'], 'r', encoding='utf-8') as f: 
            vocab_data = json.load(f)
        
        required_hashes = self.get_vocabulary_hashes(vocab_data)
        missing_files = {h for h in required_hashes if not (self.audio_dir / f"{h}.mp3").exists()}
        
        if not missing_files: 
            return {'status': 'all_files_exist', 'count': len(required_hashes)}
        
        logger.info(f"üéµ {vocab_name}: —Ç—Ä–µ–±—É–µ—Ç—Å—è {len(missing_files)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤")
        hash_to_text_map = {self._get_text_hash(lang, entry[field]): (lang, entry[field])
                            for entry in vocab_data
                            for field, lang in [('german', 'de'), ('russian', 'ru'), ('sentence', 'de'), ('sentence_ru', 'ru')]
                            if entry.get(field)}

        tasks = [self._generate_audio_file(h, *hash_to_text_map[h]) for h in missing_files if h in hash_to_text_map]
        results = await asyncio.gather(*[asyncio.wait_for(task, timeout=45) for task in tasks], return_exceptions=True)
        
        success_hashes = {h for h, r in zip(missing_files, results) if not isinstance(r, Exception)}
        if success_hashes:
            self.protected_hashes.update(success_hashes)
            self.save_manifest()
            
        return {'generated': len(success_hashes), 'failed': len(results) - len(success_hashes)}

    def get_vocabulary_hashes(self, vocab_data: List[Dict]) -> Set[str]:
        return {self._get_text_hash(l, e[f]) for e in vocab_data 
                for f,l in [('german','de'),('russian','ru'),('sentence','de'),('sentence_ru','ru')] 
                if e.get(f)}

    def _get_text_hash(self, lang: str, text: str) -> str:
        return hashlib.md5(f"{lang}:{text}".encode('utf-8')).hexdigest()

    async def _generate_audio_file(self, hash_value: str, lang: str, text: str):
        filepath = self.audio_dir / f"{hash_value}.mp3"
        if filepath.exists(): return

        async with self.gtts_semaphore:
            for attempt in range(3):
                try:
                    await asyncio.get_event_loop().run_in_executor(None, self._blocking_gtts_save, text, lang, str(filepath))
                    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {filepath.name}")
                    return
                except Exception as e:
                    logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {filepath.name} –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å: {e}")
                    if attempt < 2: await asyncio.sleep(2 ** attempt)
                    else: raise

    def _blocking_gtts_save(self, text, lang, path):
        tts = gTTS(text=text, lang=lang, slow=False)
        tts.save(path)

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã ---
auto_system = AutoVocabularySystem()

# --- API Endpoints ---
@app.route('/synthesize', methods=['GET'])
@limiter.limit("30 per minute")
def synthesize_speech():
    text, lang = request.args.get('text', '').strip(), request.args.get('lang', '').lower()
    if not (1 <= len(text) <= 500): 
        return jsonify({"error": "Invalid text length (1-500)"}), 400
    if lang not in SUPPORTED_LANGUAGES: 
        return jsonify({"error": f"Unsupported language: {lang}"}), 400
    
    hash_val = auto_system._get_text_hash(lang, text)
    filename = f"{hash_val}.mp3"
    filepath = auto_system.audio_dir / filename
    
    if not filepath.exists():
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(auto_system._generate_audio_file(hash_val, lang, text))
            logger.info(f"üé§ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {filename}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}", exc_info=not PRODUCTION)
            return jsonify({"error": f"Failed to generate audio: {e}"}), 500
    
    return jsonify({"url": f"/audio/{filename}"})

@app.route('/audio/<filename>')
@limiter.limit("120 per minute")
def serve_audio(filename):
    if not Path(auto_system.audio_dir, filename).exists():
        return jsonify({"error": "File not found"}), 404
    auto_system.record_file_access(filename)
    return send_from_directory(str(auto_system.audio_dir), filename)

@app.route('/api/vocabularies/list')
def get_vocabularies_list():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON-—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    """
    vocab_list = [
        {
            "name": name,
            "word_count": data.get('word_count', 0),
            "last_modified": data.get('last_modified', 0),
            "url": f"/api/vocabulary/{name}"
        }
        for name, data in auto_system.vocabulary_registry.items()
    ]
    
    if not vocab_list:
        logger.warning("–ó–∞–ø—Ä–æ—à–µ–Ω —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ä–µ–µ—Å—Ç—Ä–µ.")
    
    return jsonify(vocab_list)

@app.route('/api/vocabulary/<vocab_name>')
def get_vocabulary(vocab_name):
    """
    –û—Ç–¥–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–π–ª —Å–ª–æ–≤–∞—Ä—è –ø–æ –µ–≥–æ –∏–º–µ–Ω–∏ (–±–µ–∑ .json).
    """
    if vocab_name not in auto_system.vocabulary_registry:
        logger.error(f"–ü–æ–ø—ã—Ç–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –Ω–µ–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–ª–æ–≤–∞—Ä—é: {vocab_name}")
        return jsonify({"error": f"Vocabulary '{vocab_name}' not found."}), 404
    
    vocab_filename = f"{vocab_name}.json"
    logger.info(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª —Å–ª–æ–≤–∞—Ä—è: {vocab_filename} –∏–∑ {auto_system.vocabularies_dir}")
    return send_from_directory(str(auto_system.vocabularies_dir), vocab_filename)

@app.route('/health')
def health_check():
    return jsonify({
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(), 
        "gtts_semaphore_available": auto_system.gtts_semaphore._value, 
        "queue_size": auto_system.processing_queue.qsize()
    })

@app.route('/status')
def system_status():
    return jsonify({
        "system": "AutoVocabularySystem",
        "version": "1.3.1-final",
        "status": "running",
        "production_mode": PRODUCTION,
        "background_processor_active": auto_system.background_thread.is_alive(),
        "file_watcher_active": auto_system.file_observer.is_alive() if auto_system.file_observer else False,
        "config": {k: v for k, v in auto_system.config.items() if 'pattern' not in k}
    })

@app.route('/cache/stats')
def cache_stats():
    audio_files = list(auto_system.audio_dir.glob("*.mp3"))
    protected_count = sum(1 for f in audio_files if auto_system.is_file_protected(f.name))
    return jsonify({
        "total_files": len(audio_files), 
        "protected_files": protected_count, 
        "orphan_files": len(audio_files) - protected_count, 
        "total_size_mb": round(sum(f.stat().st_size for f in audio_files) / (1024 * 1024), 2) if audio_files else 0
    })

@app.route('/debug/quick')
def quick_debug():
    """–ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
    json_files_count = 0
    if auto_system.vocabularies_dir.exists():
        json_files_count = len(list(auto_system.vocabularies_dir.glob("*.json")))

    return jsonify({
        "vocabularies_dir_exists": auto_system.vocabularies_dir.exists(),
        "json_files_in_dir_count": json_files_count,
        "current_registry_count": len(auto_system.vocabulary_registry),
        "current_registry_keys": list(auto_system.vocabulary_registry.keys()),
        "exclude_patterns_in_use": auto_system.config['exclude_patterns']
    })

# --- Graceful Shutdown ---
def graceful_shutdown():
    logger.info("üõë –ò–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")
    auto_system.stop_file_watcher()
    if hasattr(auto_system, 'loop') and auto_system.loop.is_running():
        future = asyncio.run_coroutine_threadsafe(auto_system.processing_queue.put(asyncio.CancelledError()), auto_system.loop)
        try:
            future.result(timeout=2)
        except asyncio.TimeoutError:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —á–∏—Å—Ç–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ—á–µ—Ä–µ–¥—å –∑–∞–¥–∞—á.")
        auto_system.loop.call_soon_threadsafe(auto_system.loop.stop)
    
    if auto_system.background_thread.is_alive():
        auto_system.background_thread.join(timeout=3)
    logger.info("‚úÖ –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
    
def signal_handler(sig, frame):
    graceful_shutdown()
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# --- –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---
if __name__ == '__main__':
    logger.info("ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ TTS –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    
    if auto_system.config.get('cleanup_on_startup', False):
        auto_system.smart_cleanup(force=True)
    
    auto_system.scan_vocabularies()
    
    port = int(os.getenv('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)