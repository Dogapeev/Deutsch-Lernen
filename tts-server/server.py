# –§–∞–π–ª: tts-server/server.py
# –í–ï–†–°–ò–Ø 1.5.0: –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ —Å asyncio –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π threading –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.

import os
import json
import hashlib
import time
from pathlib import Path
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from gtts import gTTS
import logging
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import threading
import queue # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ—á–µ—Ä–µ–¥—å
from datetime import datetime
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import signal
import sys
import atexit

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
app = Flask(__name__)
CORS(app)
limiter = Limiter(get_remote_address, app=app, default_limits=["200 per day", "50 per hour"])

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
PRODUCTION = os.getenv('PRODUCTION', 'false').lower() == 'true'
AUDIO_DIR = "audio_cache"
VOCABULARIES_DIR = "vocabularies"
CACHE_MANIFEST = "cache_manifest.json"
CONFIG_FILE = "auto_config.json"
SUPPORTED_LANGUAGES = {'de', 'ru', 'en', 'fr', 'es'}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_level = logging.WARNING if PRODUCTION else logging.INFO
logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AutoVocabularySystem:
    def __init__(self):
        self.audio_dir = Path(AUDIO_DIR)
        self.vocabularies_dir = Path(VOCABULARIES_DIR)
        self.config = self.load_config()
        
        # –ó–∞–º–µ–Ω—è–µ–º asyncio.Queue –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ—á–µ—Ä–µ–¥—å
        self.processing_queue = queue.Queue()
        self.gtts_lock = threading.Lock()
        
        self._initialized = False
        self.file_observer = None

        os.makedirs(AUDIO_DIR, exist_ok=True)
        os.makedirs(VOCABULARIES_DIR, exist_ok=True)
        self.vocabulary_registry = self.load_manifest()
        
        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–π –ø–æ—Ç–æ–∫-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫
        self.background_thread = threading.Thread(target=self.background_processor, daemon=True)

    def ensure_initialized(self):
        if self._initialized: return
        logger.info("üöÄ –í—ã–ø–æ–ª–Ω—è—é –æ—Ç–ª–æ–∂–µ–Ω–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é...")
        if not self.background_thread.is_alive():
            self.background_thread.start()
        self.scan_vocabularies(auto_process=True)
        if self.config.get('auto_watch_enabled', True): self.start_file_watcher()
        self._initialized = True
        logger.info("‚úÖ –û—Ç–ª–æ–∂–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    def load_config(self):
        default_config = {"auto_watch_enabled": True, "auto_process_on_startup": True}
        if os.path.exists(CONFIG_FILE):
            try:
                with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                    default_config.update(json.load(f))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return default_config
    
    def load_manifest(self):
        if os.path.exists(CACHE_MANIFEST):
            try:
                with open(CACHE_MANIFEST, 'r', encoding='utf-8') as f:
                    return json.load(f).get('vocabularies', {})
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞: {e}")
        return {}

    def save_manifest(self):
        try:
            with open(CACHE_MANIFEST, 'w', encoding='utf-8') as f:
                json.dump({'vocabularies': self.vocabulary_registry}, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞: {e}")

    def scan_vocabularies(self, auto_process=None):
        logger.info("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π...")
        if auto_process is None: auto_process = self.config.get('auto_process_on_startup', True)
        
        new_or_changed = []
        for vocab_file in self.vocabularies_dir.glob("*.json"):
            try:
                vocab_name = vocab_file.stem
                current_mtime = vocab_file.stat().st_mtime
                if (vocab_name not in self.vocabulary_registry or self.vocabulary_registry[vocab_name].get('last_modified', 0) < current_mtime):
                    with open(vocab_file, 'r', encoding='utf-8') as f:
                        vocab_data = json.load(f)
                    word_count = len(vocab_data.get('words', vocab_data))
                    self.vocabulary_registry[vocab_name] = {'word_count': word_count, 'last_modified': current_mtime, 'status': 'detected'}
                    new_or_changed.append(vocab_name)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞—Ä—è {vocab_file.name}: {e}")
        
        if new_or_changed:
            self.save_manifest()
            if auto_process:
                for vocab_name in new_or_changed:
                    self.processing_queue.put({'action': 'pregenerate', 'vocab_name': vocab_name})
                logger.info(f"üîÑ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é: {new_or_changed}")
        return new_or_changed

    def background_processor(self):
        logger.info("üîÑ –§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (threading) –∑–∞–ø—É—â–µ–Ω")
        while True:
            try:
                task = self.processing_queue.get() # –ë–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è, –ø–æ–∫–∞ –≤ –æ—á–µ—Ä–µ–¥–∏ –Ω–µ –ø–æ—è–≤–∏—Ç—Å—è –∑–∞–¥–∞—á–∞
                if task is None: # –°–∏–≥–Ω–∞–ª –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
                    logger.info("–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–æ–ª—É—á–∏–ª —Å–∏–≥–Ω–∞–ª –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É.")
                    break
                
                if task.get('action') == 'pregenerate':
                    vocab_name = task['vocab_name']
                    logger.info(f"üéµ –ù–∞—á–∏–Ω–∞–µ–º –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è: {vocab_name}")
                    self.pregenerate_vocabulary_audio(vocab_name)
                    self.vocabulary_registry.setdefault(vocab_name, {})['status'] = 'ready'
                    self.save_manifest()
                    logger.info(f"‚úÖ –ê–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è {vocab_name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                
                self.processing_queue.task_done()
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Ñ–æ–Ω–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ: {e}", exc_info=not PRODUCTION)
                time.sleep(10)

    def start_file_watcher(self):
        if self.file_observer and self.file_observer.is_alive(): return
        class VocabularyFileHandler(FileSystemEventHandler):
            def __init__(self, system_ref): self.system, self.timer = system_ref, None
            def on_any_event(self, event):
                if not event.is_directory and event.src_path.endswith(".json"):
                    if self.timer: self.timer.cancel()
                    self.timer = threading.Timer(2.0, self.system.scan_vocabularies, args=[True]); self.timer.start()
        
        self.file_observer = Observer()
        self.file_observer.schedule(VocabularyFileHandler(self), str(self.vocabularies_dir), recursive=True)
        self.file_observer.start()
        logger.info(f"üëÅÔ∏è –ó–∞–ø—É—â–µ–Ω–æ —Å–ª–µ–∂–µ–Ω–∏–µ –∑–∞ –ø–∞–ø–∫–æ–π: {self.vocabularies_dir}")
        
    def stop_file_watcher(self):
        if self.file_observer and self.file_observer.is_alive():
            self.file_observer.stop(); self.file_observer.join()
            logger.info("üëÅÔ∏è –°–ª–µ–∂–µ–Ω–∏–µ –∑–∞ —Ñ–∞–π–ª–∞–º–∏ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")

    def pregenerate_vocabulary_audio(self, vocab_name: str):
        with open(Path(self.vocabularies_dir, f"{vocab_name}.json"), 'r', encoding='utf-8') as f:
            vocab_data = json.load(f)
        
        words_list = vocab_data.get('words', vocab_data)
        
        for entry in words_list:
            for field, lang in [('german', 'de'), ('russian', 'ru'), ('sentence', 'de'), ('sentence_ru', 'ru')]:
                if text := entry.get(field):
                    self.generate_audio_sync(lang, text)

    def _get_text_hash(self, lang: str, text: str) -> str:
        return hashlib.md5(f"{lang}:{text}".encode('utf-8')).hexdigest()

    def _blocking_gtts_save(self, text, lang, path):
        with self.gtts_lock: # –ó–∞–º–æ–∫ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø–æ—Ç–æ–∫ –≤—ã–∑—ã–≤–∞–µ—Ç gTTS
            tts = gTTS(text=text, lang=lang, slow=False)
            tts.save(path)

    def generate_audio_sync(self, lang: str, text: str):
        hash_val = self._get_text_hash(lang, text)
        filepath = self.audio_dir / f"{hash_val}.mp3"
        if filepath.exists(): return
        
        for attempt in range(1, 4):
            try:
                self._blocking_gtts_save(text, lang, str(filepath))
                logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {filepath.name}")
                return
            except Exception as e:
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt} –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {filepath.name} –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å: {e}")
                if attempt < 3: time.sleep(attempt * 2)
                else: logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å {filepath.name} –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫.")

auto_system = AutoVocabularySystem()

@app.route('/synthesize', methods=['GET'])
@limiter.limit("30 per minute")
def synthesize_speech():
    auto_system.ensure_initialized()
    text = request.args.get('text', '').strip()
    lang = request.args.get('lang', '').lower()
    if not (1 < len(text) <= 500 and lang in SUPPORTED_LANGUAGES):
        return jsonify({"error": "Invalid input"}), 400
    
    try:
        auto_system.generate_audio_sync(lang, text)
        hash_val = auto_system._get_text_hash(lang, text)
        return jsonify({"url": f"/audio/{hash_val}.mp3"})
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return jsonify({"error": "Failed to generate audio"}), 500

@app.route('/audio/<filename>')
def serve_audio(filename):
    return send_from_directory(str(auto_system.audio_dir), filename)

@app.route('/api/vocabularies/list')
def get_vocabularies_list():
    auto_system.ensure_initialized()
    return jsonify([{"name": name, "word_count": data.get('word_count', 0)} for name, data in auto_system.vocabulary_registry.items()])

@app.route('/api/vocabulary/<vocab_name>')
def get_vocabulary(vocab_name):
    auto_system.ensure_initialized()
    return send_from_directory(str(auto_system.vocabularies_dir), f"{vocab_name}.json")

@app.route('/status')
def system_status():
    return jsonify({"version": "1.5.0-threading-stable", "initialized": auto_system._initialized})

def graceful_shutdown():
    logger.info("üõë –ò–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞...")
    auto_system.stop_file_watcher()
    if auto_system.background_thread.is_alive():
        auto_system.processing_queue.put(None) # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–∏–≥–Ω–∞–ª –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        auto_system.background_thread.join(timeout=5) # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞
    logger.info("‚úÖ –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")

atexit.register(graceful_shutdown)
signal.signal(signal.SIGINT, lambda s, f: sys.exit(0))
signal.signal(signal.SIGTERM, lambda s, f: sys.exit(0))

if __name__ == '__main__':
    logger.info("ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ TTS –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    auto_system.ensure_initialized()
    port = int(os.getenv('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)